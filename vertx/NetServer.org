
* NetServer

** TCPServerBase
#+begin_src java
public abstract class TCPServerBase implements Closeable, MetricsProvider {

  protected final Context creatingContext;
  protected final VertxInteral vertx;
  protected final NetServerOptionso options;

  // Per Server
  protected EventLoop eventLoop;
  private Handler<Channel> worker;
  // Server 监听状态, 已经监听则为true;
  private volatile boolean listening;
  private ContextInternal listenContext;
  private TCPServerBase actualServer;

  // Main
  private SSLHelper sslHelper;
  private ServerChannelLoadBalancer channelBalancer;
  private Future<Channel> bindFuture;
  private Set<TCPServerBase> servers;
  private TCPMetrices<?> metrics;
  private volatile int actualPort;

  public TCPServerBase(VertxInternal vertx, NetServerOptions options) {
    this.vertx = vertx;
    this.options = new NetServerOptions(options);
    this.creatingContext = vertx.getContext();
  }
  
}
#+end_src

#+begin_src java
private synchronized Future<Channel> listen(SocketAddress localAddress, ContextInternal context) {
  // 首先判断是否已经监听过了
  if (listening) {
    throw new IllegalStateException("Listen already called");
  }

  this.listenContext = context;
  this.listening = true;
  // 从context 中获取关联的 eventLoop
  this.eventLoop = context.nettyEventLoop();

  SocketAddress bindAddress;
  Map<ServerID, TCPServerBase> sharedNetServers = vertx.sharedTCPServers((Class<TCPServerBase>) getClass());
  synchronized (sharedNetServers) {
    actualPort = localAddress.port();
    String hostOrPath = localAddress.isInetSocket() ? localAddress.host() : localAddress.path();
    TCPServerBase main;
    boolean shared;
    ServerID id;
    if (actualPort > 0 || localAddress.isDomainSocket()) {
      id = new ServerID(actualPort, hostOrPath);
      main = sharedNetServers.get(id);
      shared = true;
      bindAddress = localAddress;
    } else {
      if (actualPort < 0) {
        id = new ServerID(actualPort, hostOrPath + "/" + -actualPort);
        main = sharedNetServers.get(id);
        shared = true;
        bindAddress = SocketAddress.inetSocketAddress(0, localAddress.host());
      } else {
        id = new ServerID(actualPort, hostOrPath);
        main = null;
        shared = false;
        bindAddress = localAddress;
      }
    }
    PromiseInternal<Channel> promise = listenContext.promise();
    if (main == null) {
      // The first server binds the socket
      actualServer = this;
      bindFuture = promise;
      sslHelper = createSSLHelper();
      worker =  childHandler(listenContext, localAddress, sslHelper);
      servers = new HashSet<>();
      servers.add(this);
      // accpectorEventLoopGroup中只有一个EventLoop. 
      // 所以其实所有的Server Socket IO请求都是由同一个EventLoop处理的.
      // 由所以它的ioRatio 设置为了 100
      channelBalancer = new ServerChannelLoadBalancer(vertx.getAcceptorEventLoopGroup().next());

      // Register the server in the shared server list
      if (shared) {
        sharedNetServers.put(id, this);
      }

      listenContext.addCloseHook(this);

      // Initialize SSL before binding
      sslHelper.init(listenContext).onComplete(ar -> {
        if (ar.succeeded()) {
          // 在SSL初始化完成后
          // Socket bind
          channelBalancer.addWorker(eventLoop, worker);
          // Netty 的部分
          ServerBootstrap bootstrap = new ServerBootstrap();
          // 注意这里的childGroup 参数是 ChannelBalancer 里的 VertxEventLoopGroup
          // 是一个手动添加EventLoop的动态EventLoopGroup.
          // 也就是说首次调用时, 这个 childGroup 中只有一个EventLoop
          // 所有的连接都在这一个EventLoop上处理.
          // 结合ShareServer机制可以用于自定义使用的EventLoop个数.
          bootstrap.group(vertx.getAcceptorEventLoopGroup(), channelBalancer.workers());
          if (sslHelper.isSSL()) {
            bootstrap.childOption(ChannelOption.ALLOCATOR, PartialPooledByteBufAllocator.INSTANCE);
          } else {
            bootstrap.childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT);
          }

          // ChannelBalancer 重点对象
          bootstrap.childHandler(channelBalancer);
          applyConnectionOptions(localAddress.isDomainSocket(), bootstrap);

          // Actual bind
          io.netty.util.concurrent.Future<Channel> bindFuture = AsyncResolveConnectHelper.doBind(vertx, bindAddress, bootstrap);
          bindFuture.addListener((GenericFutureListener<io.netty.util.concurrent.Future<Channel>>) res -> {
            if (res.isSuccess()) {
              Channel ch = res.getNow();
              log.trace("Net server listening on " + hostOrPath + ":" + ch.localAddress());
              if (shared) {
                ch.closeFuture().addListener((ChannelFutureListener) channelFuture -> {
                  synchronized (sharedNetServers) {
                    sharedNetServers.remove(id);
                  }
                });
              }
              // Update port to actual port when it is not a domain socket as wildcard port 0 might have been used
              if (bindAddress.isInetSocket()) {
                actualPort = ((InetSocketAddress)ch.localAddress()).getPort();
              }
              metrics = createMetrics(localAddress);
              promise.complete(ch);
            } else {
              promise.fail(res.cause());
            }
          });
        } else {
          promise.fail(ar.cause());
        }
      });

      bindFuture.onFailure(err -> {
        if (shared) {
          synchronized (sharedNetServers) {
            sharedNetServers.remove(id);
          }
        }
        listening = false;
      });

      return bindFuture;
    } else {
      // Server already exists with that host/port - we will use that
      actualServer = main;
      metrics = main.metrics;
      sslHelper = main.sslHelper;
      worker =  childHandler(listenContext, localAddress, sslHelper);
      actualServer.servers.add(this);
      actualServer.channelBalancer.addWorker(eventLoop, worker);
      listenContext.addCloseHook(this);
      main.bindFuture.onComplete(promise);
      return promise.future();
    }
  }
}
#+end_src

#+begin_src java
class ServerChannelLoadBalancer extends ChannelInitializer<Channel> {

  private final VertxEventLoopGroup workers;
  private final ChannelGroup channelGroup;
  // 
  private final ConcurrentMap<EventLoop, WorkerList> workerMap = new ConcurrentHashMap<>();

  // We maintain a separate hasHandlers variable so we can implement hasHandlers() efficiently
  // As it is called for every HTTP message received
  private volatile boolean hasHandlers;

  // 唯一的调用是 TCPserverBase.java 141 channelBalancer = new ServerChannelLoadBalancer(vertx.getAcceptorEventLoopGroup().next());
  // 所以这里其实就是vertx中的唯一用来处理ServerSocket accept事件的EventLoop(Acceptor).
  ServerChannelLoadBalancer(EventExecutor executor) {
    // 这里 Vertx 自己实现了一个简单的 EventLoopGroup逻辑
    // 可以添加的 EventLoop 的 EventLoopGroup.
    // 简单的说就是用了一个 List, 保持真正的EventLoop, 并且提供 add 和 remove 方法
    this.workers = new VertxEventLoopGroup();
    // 用了 Netty ChannelGroup. 每个子线程添加到其中. 
    // 然后在server stop时可以统一close
    this.channelGroup = new DefaultChannelGroup(executor);
  }

  @Override
  protected void initChannel(Channel ch) {
    // 通过 channel 绑定的EventLoop 获取对应的处理逻辑.
    // 这里的 worker 是ServerBootstrap的childGroup.
    Handler<Channel> handler = chooseInitializer(ch.eventLoop());
    if (handler == null) {
      ch.close();
    } else {
      channelGroup.add(ch);
      handler.handle(ch);
    }
  }

  private Handler<Channel> chooseInitializer(EventLoop worker) {
    WorkerList handlers = workerMap.get(worker);
    return handlers == null ? null : handlers.chooseHandler();
  }

  /**
  * EventLoop Channel的处理线程
  * handler 是处理Channel的逻辑
  */
  public synchronized void addWorker(EventLoop eventLoop, Handler<Channel> handler) {
    // 1. 先把 eventLoop 添加到 eventLoopGroup中
    workers.addWorker(eventLoop);
    WorkerList handlers = new WorkerList();
    WorkerList prev = workerMap.putIfAbsent(eventLoop, handlers);
    if (prev != null) {
      handlers = prev;
    }
    handlers.addWorker(handler);
    hasHandlers = true;
  }

  public synchronized boolean removeWorker(EventLoop worker, Handler<Channel> handler) {
    WorkerList handlers = workerMap.get(worker);
    if (handlers == null || !handlers.removeWorker(handler)) {
      return false;
    }
    if (handlers.isEmpty()) {
      workerMap.remove(worker);
    }
    if (workerMap.isEmpty()) {
      hasHandlers = false;
    }
    //Available workers does it's own reference counting -since workers can be shared across different Handlers
    workers.removeWorker(worker);
    return true;
  }

}
#+end_src
